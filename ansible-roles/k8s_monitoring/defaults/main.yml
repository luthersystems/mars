---
# defaults file for k8s_monitoring

k8s_monitoring_root_path: "{{kubectl_asset_root_path}}/monitoring"
k8s_monitoring_helm_release: monitoring
k8s_monitoring_namespace: monitoring

grafana_admin_user: luther
grafana_admin_password: !!null

grafana_google_client_id: !!null
grafana_google_client_secret: !!null

# grafana_custom_oauth_config is a dict with the auth.generic_oauth structure
# specified in the grafana docs.  The client_id and client_secret will be
# stored in an k8s secret resource.
#
#   https://grafana.com/docs/grafana/latest/auth/generic-oauth
#
grafana_custom_oauth_config: {}

grafana_frontend_url: http://localhost:9292/
grafana_frontend_certificate_arn: arn:aws:acm:eu-central-1:967058059066:certificate/c37d5e83-3b1b-41bd-a109-81827d956b5e

# grafana_custom_dashboards maps filename (*.json) to dashboard objects.
grafana_custom_dashboards: {}
grafana_default_dashboards: "{{lookup('grafana_dashboard_dir', '/opt/grafana-dashboards')}}"
grafana_all_dashboards: "{{grafana_default_dashboards | combine(grafana_custom_dashboards)}}"

# prometheus_server_custom_rule_groups maps group names to array of rule
# objects. See prometheus_server_default_rule_groups below.
prometheus_server_custom_rule_groups: {}
prometheus_server_all_rule_groups: "{{prometheus_server_default_rule_groups|combine(prometheus_server_custom_rule_groups)}}"
prometheus_server_global_scrape_interval: 30s
prometheus_server_global_scrape_timeout: 10s

prometheus_server_external_hostname: prometheus.luther.systems
prometheus_server_load_balancer_source_ranges:
  - "10.0.0.0/8"

prometheus_alertmanager_slack_api_url: !!null

prometheus_alertmanager_configmap_data:
  alertmanager.yml: |
    ---
    global:
      slack_api_url: "{{prometheus_alertmanager_slack_api_url}}"

    receivers:
      - name: slack_alerts
        slack_configs:
        - send_resolved: true
          channel: "#alerts"
          title: '{% raw %}{{ template "slack.luther.title" . }}{% endraw %}'
          text: '{% raw %}{{ template "slack.luther.text" . }}{% endraw %}'

    templates:
      - /etc/config/*.tmpl

    route:
      receiver: slack_alerts
      group_wait: 15s
      group_interval: 5m
      group_by:
        - project
        - environment
        - alertname
  slack.luther.text.tmpl: !unsafe |
    {{ define "slack.luther.text" -}}
    {{ with $data := . -}}

    {{ if eq .Status "firing" -}}
    <!channel>: :scream_cat: {{len .Alerts.Firing}} active alerts in the group{{range .Alerts.Firing}} :fire:{{end}}
    {{- else -}}
    :disappointed_relieved: All alerts in the group are resolved
    {{- end }}

    {{- range $ann := .CommonAnnotations.SortedPairs }}
    - *{{.Name}}*: {{.Value}}
    {{- end }}
    {{- if gt (len .Alerts) 1 }}
    {{- range $alert := .Alerts }}
    {{ if eq .Status "firing" }}:fire:{{ else }}:ok_hand:{{ end }} {{(.Labels.Remove $data.CommonLabels.Names).Values | join " "}}
    {{- range $ann := (.Annotations.Remove $data.CommonAnnotations.Names).SortedPairs}}
    - *{{.Name}}*: {{.Value}}
    {{- end }}
    {{- end }}
    {{- end }}
    {{- end }}
    {{- end }}
  slack.luther.title.tmpl: !unsafe |
    {{ define "slack.luther.title" -}}
    [{{ .GroupLabels.project }}-{{ .GroupLabels.environment }}]
    {{- range .GroupLabels.SortedPairs }}
    {{- if or (eq .Name "project") (eq .Name "environment") }}{{ else }} {{ .Value }}{{ end }}{{ end }}
    {{- if gt (len .CommonLabels) (len .GroupLabels)}} ({{(.CommonLabels.Remove .GroupLabels.Names).Values | join " "}}){{end}}
    {{- end }}

k8s_monitoring_chart_values:
  prometheus:
    server:
      global:
        scrape_interval: "{{prometheus_server_global_scrape_interval}}"
        scrape_timeout: "{{prometheus_server_global_scrape_timeout}}"
      service:
        type: LoadBalancer
        loadBalancerSourceRanges: "{{prometheus_server_load_balancer_source_ranges}}"
        annotations:
          service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
          service.beta.kubernetes.io/aws-load-balancer-internal: "true"
          service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: Project={{luther_project_id}},Environment={{luther_env}},Organization={{org_name}},Component=mon,Resource=alb,Subcomponent=prom
          external-dns.alpha.kubernetes.io/hostname: "{{prometheus_server_external_hostname}}"
    serverFiles:
      alerts: "{{prometheus_server_all_rule_groups}}"
    alertmanager:
      configMapOverrideName: "{{prometheus_alertmanager_configmap_name}}"
  grafana:
    admin:
      existingSecret: "{{grafana_credentials_secret_name}}"
    envFromSecret: "{{grafana_env_secret_name}}"
    ingress:
      enabled: true
      hosts:
        - "{{grafana_frontend_url | urlsplit('hostname')}}"
      path: /*
      extraPaths:
        - path: /*
          backend:
            serviceName: ssl-redirect
            servicePort: use-annotation
      annotations:
        alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
        alb.ingress.kubernetes.io/actions.ssl-redirect: '{"Type": "redirect", "RedirectConfig": { "Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}'
        alb.ingress.kubernetes.io/backend-protocol: HTTP
        alb.ingress.kubernetes.io/certificate-arn: '{{grafana_frontend_certificate_arn}}'
        alb.ingress.kubernetes.io/scheme: internet-facing
        alb.ingress.kubernetes.io/target-type: ip
        alb.ingress.kubernetes.io/ssl-policy: "ELBSecurityPolicy-TLS-1-2-2017-01"
        kubernetes.io/ingress.class: alb
        alb.ingress.kubernetes.io/tags: Project={{luther_project_id}},Environment={{luther_env}},Organization={{org_name}},Component=mon,Resource=alb
    grafana.ini:
      server:
        root_url: '{{grafana_frontend_url}}'
      auth.basic:
        enabled: false
      auth.generic_oauth: "{{ {'enabled': False } | combine(grafana_custom_oauth_config) | dict_without_keys(['client_id','client_secret']) }}"
      auth.google:
        enabled: true
        scopes: https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email
        auth_url: https://accounts.google.com/o/oauth2/auth
        token_url: https://accounts.google.com/o/oauth2/token
        allowed_domains: luthersystems.com
        allow_sign_up: true
      analytics:
        reporting_enabled: false
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Default
            type: prometheus
            access: proxy
            url: "http://{{k8s_monitoring_helm_release}}-prometheus-server/"
            isDefault: true
            editable: false
    dashboardProviders:
      # dashboardproviders.yaml is a special name that must be used
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: Defaults
            folder: defaults
            type: file
            disableDeletion: true
            editable: true
            updateIntervalSeconds: 60
            allowUiUpdates: true
            options:
              path: /var/lib/grafana/dashboards
    dashboardsConfigMaps:
      defaults: "{{grafana_dashboard_configmap_name}}"

prometheus_server_default_rule_groups:
  service:
    - alert: LowDataVolumeSpace
      # used > 80%
      expr: (kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes) < 0.2
      for: 5m
      labels:
        project: "{{luther_project_name}}"
        environment: "{{luther_env}}"
        severity: page
      annotations:
        summary: "Service data volume usage above 80%"
    - alert: LowRootVolumeSpace
      # used > 80%
      expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) < 0.2
      for: 1m
      labels:
        project: "{{luther_project_name}}"
        environment: "{{luther_env}}"
        severity: page
      annotations:
        summary: "Instance(s) root volume usage above 80%"
    - alert: LowInstanceMemory
      # used > 80%
      expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) < 0.2
      for: 1m
      labels:
        project: "{{luther_project_name}}"
        environment: "{{luther_env}}"
        severity: page
      annotations:
        summary: "Instance(s) memory usage above 80%"
    - alert: ServiceDown
      expr: up != 1
      for: 2m
      labels:
        project: "{{luther_project_name}}"
        environment: "{{luther_env}}"
        severity: page
      annotations:
        summary: "Service(s) could not be scraped"
